### MEMM到CRF
- MEMM 打破了观测独立假设是更加合理的
- MEMM 的局部归一化问题让观测值对隐变量的印象几乎没有。熵越低，概率越高，隐变量的值越不受观测值的影响。可以看作一个confidence，1的话即完全相信上一次的预测。解决办法是把有向图模型变成无向图模型，顺便也打破了齐次马尔科夫假设。
- 也就是

```
graph LR
MEMM-->CRF
有向-->无向
```
### MEMM的问题所在
- 每一个小模型可以类似于一个逻辑回归。泛化。
- y_{t-1}到y_t是有一个概率的，也就是一个概率分布。然后x_t或者x_{1-T}可以看作一个输入。
- 局部产生的效果叫做MASS SCORE。来自PDF（连续）PMF（离散）。
- MASS SCORE 被归一化了。三个整体看成一个函数，正的。
- 三者形成的小单体实际上是一个概率分布，所以必须要归一化，让所有的概率加起来等于1。通俗的理解就是两个输入都对输出有影响但是影响的概率之和为1。
- 1每一个概率都要归一化就会出现问题。假如极端情况，采用Veterbi算法，argmax之后直接会选用采样概率更大的那一个，从而不会去关心之后的条件概率的条件（假设条件是概率小的那个，也就是想学习到的模型是采样概率小的的那个，但是argmax之后还是会选择概率大的那个）。
- 归一化的问题就是唯概率是论，从而不关心条件。也就是条件概率分布熵很小，也就是原来模型的概率很大，就会导致对观测变量的忽视。

---

### 算法复杂度的计算
==常数级别：1==\
普通语句\
==对数级别：logN==\
二分查找\
==线性级别：N==\
循环，找出最大元素，简单的连乘，连加\
==线性对数级别：NlogN==\
分治，归并排序\
==平方级别：N^2== \
双层循环，检查所有元素对\
==立方级别：N^3== \
三层循环，检查所有三元组\
==指数级别：2^N== \
穷举查找 检查所有子集，类似于连加函数，连乘函数


