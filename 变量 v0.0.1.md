# Sparse
Reawrds shaping 也就是所谓的调参！！！！！
将单纯的最大化Rewards变成最大化Rewards和ICModule，也就是在探索和利用之间的平衡。
- curosity driven (ICM)
- 于Kalman Filter 的关系
    - 同样是拥有观测值（真实值）和一个预测值
    - KF是去选择观测值和真实值那个最可信，预测与现实情况差别的大小决定预测的可信度大小
    - ICM预测的结果和真实结果差别越大奖励越大，鼓励Machine去冒险
- 问题
    - 很难被预测并不代表结果是好的，难预测的事情很多
    - 光有好奇心是不够的--> 什么事情是真正重要的
- 怎么办
    - 加一个Feature Extractor，来过滤掉没有意义的画面
    - Features 从FE中出来之后进入另外一个Network（Network2）来判断
        - 也就是在额外的Network中，预测的值越精准说明相关程度越大。
# Curriculum Learning
规划课程，从最简单到最难的
## Reverse Curriculum Generation
- Given a goal state
- Sample some state close to goal state
- Delete goal state with too large rewards or too little rewards(Too easy or too difficult for machine to learn)
- Sample some states from the sample states
- 从目标去反推，所以叫Reverse
#  Hierarchical Reinforcement Learning
network， train不起来，train很多次，重新design network很多次。自顶向下。
- 当上面的agent提出的愿景下面的完成不了的时候呢，上面的agent就会得到惩罚。
# Imitation Learning
也叫Apprenticeship Learning,有两个Approach
## Behavior Cloning
跟Supervised Learning是一样的,但是局限性也非常的大。因为学习的样本本身的局限性就非常的大
### Dataset Aggregation
就是缺啥补啥，但是也是勉勉强强的修补。主要问题是哪怕学的差了一点，一点一点积累起来，差之毫厘，谬以千里。
## Inverse Reinforcement Learning (IRL)
也叫Inverse Optimal Control,根据行为反推Rewards Function。简单的Rewards Function也可以推导出来非常复杂的行为。先射箭再画靶。The experts is always the best.
### 如何实现呢?
通过比较Reward Function， Expert的reward应该永远是最大的，改原来的Rewards Function来使Re>R，最后改完的Rewards Function就是IRL需要的。
### 终止条件？
最简单就是循环个500次然后看看结果。
###与GAN的对比
Actor就是Generator，Reward Function就是Discriminitor