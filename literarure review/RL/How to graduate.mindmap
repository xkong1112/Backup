{"nodes":[{"id":"root","topic":"A*","isroot":true,"expanded":true,"parentid":null,"style":{"left":"100px","top":"332px","display":"","visibility":"visible"},"customStyle":{},"expanderStyle":{},"view":{"width":"85px","height":"66px"},"images":[],"link":{}},{"id":"e56a041dff2ee9ae","topic":"Reinforcemnet Learning","isroot":false,"expanded":true,"parentid":"root","style":{"left":"219px","top":"314px","display":"","visibility":"visible"},"customStyle":{"fontSize":"16px"},"expanderStyle":{"left":"418px","top":"320px","display":"","visibility":"visible"},"view":{"width":"199px","height":"23px"},"images":[],"link":{}},{"id":"e56a08e3ddfe5c82","topic":"Deep Q","isroot":false,"expanded":true,"parentid":"e56a041dff2ee9ae","style":{"left":"465px","top":"62px","display":"","visibility":"visible"},"customStyle":{"fontSize":"16px"},"expanderStyle":{"display":"none","visibility":"hidden"},"view":{"width":"68px","height":"23px"},"images":[],"link":{}},{"id":"e56a0a6a4c5cec64","topic":"A3C(Asynchronous Advantage Actor Critic)","isroot":false,"expanded":true,"parentid":"e56a041dff2ee9ae","style":{"left":"465px","top":"120px","display":"","visibility":"visible"},"customStyle":{"fontSize":"16px"},"expanderStyle":{"left":"818px","top":"126px","display":"","visibility":"visible"},"view":{"width":"353px","height":"23px"},"images":[],"link":{}},{"id":"e56a310e88ab4d27","topic":"With A3C, as with the rest of actor-critic methods, we learn two different functions: the policy (or “actor”), and the value (the “critic”). ","isroot":false,"expanded":true,"parentid":"e56a0a6a4c5cec64","style":{"left":"865px","top":"101px","display":"","visibility":"visible"},"customStyle":{"fontSize":"16px"},"expanderStyle":{"display":"none","visibility":"hidden"},"view":{"width":"388px","height":"62px"},"images":[],"link":{}},{"id":"e56a0b8e7841a433","topic":"HRL(Hierarchical Reinforcement Learning)","isroot":false,"expanded":true,"parentid":"e56a041dff2ee9ae","style":{"left":"465px","top":"179px","display":"","visibility":"visible"},"customStyle":{"fontSize":"16px"},"expanderStyle":{"left":"811px","top":"184.5px","display":"","visibility":"visible"},"view":{"width":"346px","height":"23px"},"images":[],"link":{}},{"id":"e56a99ac7cb29159","topic":"HIRO","isroot":false,"expanded":true,"parentid":"e56a0b8e7841a433","style":{"left":"858px","top":"179px","display":"","visibility":"visible"},"customStyle":{"fontSize":"16px"},"expanderStyle":{"display":"none","visibility":"hidden"},"view":{"width":"50px","height":"23px"},"images":[],"link":{}},{"id":"e56a9a5e5c7d9250","topic":"DDPG (Deep Deterministic Policy Gradient)","isroot":false,"expanded":true,"parentid":"e56a041dff2ee9ae","style":{"left":"465px","top":"218px","display":"","visibility":"visible"},"customStyle":{"fontSize":"16px"},"expanderStyle":{"display":"none","visibility":"hidden"},"view":{"width":"354px","height":"23px"},"images":[],"link":{}},{"id":"e56b7c26a79e94e6","topic":"Merlin","isroot":false,"expanded":true,"parentid":"e56a041dff2ee9ae","style":{"left":"465px","top":"412px","display":"","visibility":"visible"},"customStyle":{"fontSize":"16px"},"expanderStyle":{"left":"522px","top":"417.5px","display":"","visibility":"visible"},"view":{"width":"57px","height":"23px"},"images":[],"link":{}},{"id":"e56b7e3d449aa297","topic":"memory-based predictor (MBP)","isroot":false,"expanded":true,"parentid":"e56b7c26a79e94e6","style":{"left":"569px","top":"392px","display":"","visibility":"visible"},"customStyle":{"fontSize":"16px"},"expanderStyle":{"left":"830px","top":"398px","display":"","visibility":"visible"},"view":{"width":"261px","height":"23px"},"images":[],"link":{}},{"id":"e56b8ac7c07cf3a6","topic":"The MBP is responsible for compressing observations into useful, low-dimensional “state variables” to store directly into a key-value memory matrix. ","isroot":false,"expanded":true,"parentid":"e56b7e3d449aa297","style":{"left":"877px","top":"315px","display":"","visibility":"visible"},"customStyle":{"fontSize":"16px"},"expanderStyle":{"left":"1265px","top":"349.5px","display":"","visibility":"visible"},"view":{"width":"388px","height":"81px"},"images":[],"link":{}},{"id":"e56b99fe7478ac28","topic":"compressing the observation into a useful state variable z_t to pass on to the policy","isroot":false,"expanded":true,"parentid":"e56b8ac7c07cf3a6","style":{"left":"1312px","top":"257px","display":"","visibility":"visible"},"customStyle":{"fontSize":"16px"},"expanderStyle":{"display":"none","visibility":"hidden"},"view":{"width":"137px","height":"119px"},"images":[],"link":{}},{"id":"e56ba185201b95c8","topic":"writing z_t into a memory matrix","isroot":false,"expanded":true,"parentid":"e56b8ac7c07cf3a6","style":{"left":"1312px","top":"392px","display":"","visibility":"visible"},"customStyle":{"fontSize":"16px"},"expanderStyle":{"display":"none","visibility":"hidden"},"view":{"width":"137px","height":"62px"},"images":[],"link":{}},{"id":"e56b8e2d0077c3cf","topic":"It is also responsible for passing relevant memories to the policy, which uses those memories and the current state to output actions.","isroot":false,"expanded":true,"parentid":"e56b7e3d449aa297","style":{"left":"877px","top":"479px","display":"","visibility":"visible"},"customStyle":{"fontSize":"16px"},"expanderStyle":{"left":"1265px","top":"504.5px","display":"","visibility":"visible"},"view":{"width":"388px","height":"62px"},"images":[],"link":{}},{"id":"e56ba51bae88ce90","topic":"fetching other useful memories to pass on to the policy","isroot":false,"expanded":true,"parentid":"e56b8e2d0077c3cf","style":{"left":"1312px","top":"470px","display":"","visibility":"visible"},"customStyle":{"fontSize":"16px"},"expanderStyle":{"display":"none","visibility":"hidden"},"view":{"width":"137px","height":"81px"},"images":[],"link":{}},{"id":"e56b815937c76e29","topic":"policy network","isroot":false,"expanded":true,"parentid":"e56b7c26a79e94e6","style":{"left":"569px","top":"567px","display":"","visibility":"visible"},"customStyle":{"fontSize":"16px"},"expanderStyle":{"display":"none","visibility":"hidden"},"view":{"width":"125px","height":"23px"},"images":[],"link":{}},{"id":"e56a0468239ce962","topic":"Deep learning ","isroot":false,"expanded":true,"parentid":"root","style":{"left":"219px","top":"625px","display":"","visibility":"visible"},"customStyle":{"fontSize":"16px"},"expanderStyle":{"left":"345px","top":"631px","display":"","visibility":"visible"},"view":{"width":"126px","height":"23px"},"images":[],"link":{}},{"id":"e56a0d7fe3a36eaf","topic":"LSTM","isroot":false,"expanded":true,"parentid":"e56a0468239ce962","style":{"left":"392px","top":"606px","display":"","visibility":"visible"},"customStyle":{"fontSize":"16px"},"expanderStyle":{"display":"none","visibility":"hidden"},"view":{"width":"51px","height":"23px"},"images":[],"link":{}},{"id":"e56a0e7408a15906","topic":"RNN","isroot":false,"expanded":true,"parentid":"e56a0468239ce962","style":{"left":"392px","top":"645px","display":"","visibility":"visible"},"customStyle":{"fontSize":"16px"},"expanderStyle":{"display":"none","visibility":"hidden"},"view":{"width":"43px","height":"23px"},"images":[],"link":{}}],"remarks":{},"resourceList":"","zoom":1}