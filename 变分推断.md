## 变分推断
L(q)是对q的变分。q是一个概率密度函数。
KL

### 平均场理论（Mean field theory）
q划分成M个组，每个组之间相互独立，问题很大（神经网络）intractable（后验要积分就够太复杂不好求，Classical VI需要求除j以外的期望，这个期望本身也不好求）
其实就是Coordinate Ascend，可以看做坐标上升算法。（先固定一部分然后迭代，判停）

### 随机梯度变分推断
SGVI(SGVB(贝叶斯))
步长，方向？

#### 如何求梯度，积分和梯度的关系
采样本身会有误差，是一个近似，然后用参数来替代概率分布也会引入误差，导致对参数采样精度很差，方差越来越大。

#### 如何降低方差，重参数化技巧
转移不确定性到一个给定的概率分布上。可以定性地认为给定的这个概率分布和原来的是等价的，两个概率分布的积分是相等的。


## MCMC采样

## 隐马尔可夫模型HMM
### 一二三
#### 一个模型
lambda = Π A B
#### 两个假设
##### 1. 齐次Markov假设
未来状态仅与当前状态有关，与过去的状态无关。也就是无后效性。
##### 2. 观测独立假设
观测变量仅与同一时刻的状态变量有关，与其他的变量无关
##### 假设的作用以及D 分割
如果当前的状态变量是已知，也就是在条件里面。那么观测变量与下一时刻的状态变量是独立的。
#### 三个问题
##### Evaluation
也就是在已知状态变量和发射关系的情况下推观测变量，来评价模型是不是和观测变量相符合。
##### Learning
知道观测的状态和隐状态，然后来求之间的发射关系。
##### Decoding
Viterbi算法，给定观测序列然后预测隐状态序列。如果将隐状态到观测状态成为编码，相对应的从观测状态推测隐状态就叫解码。

每个状态变量有N个状态，然后从T个状态变量中找到每个状态变量的N个状态中最大的那个，连起来一个序列就是我们需要找到的序列。如果把每两个状态变量之间的最大概率类比到每两个节点之间的最短距离，那么最后找的的序列其实是遍历过所有N^T个状态之后找到的。利用max函数来实现需找到最大的这个功能，然后通过argmax来记录每个状态变量对应的节点。所以可以类比成一个动态规划问题。定量的话可以假设距离d=1/p，概率越大距离越短。

预测问题一般指的是预测之后的时刻，而解码则指的是当前时刻。

## 卡尔曼滤波
### 线性和高斯的来源
线性来自于当前状态和未来状态，以及当前状态和当前观测之间的线性关系。而对于两个噪声来说，是服从一个均值为0，方差为 Q R 的高斯分布的。
### 状态变量
A B C D Q R μ1 Σ1
其中μ1 Σ1是初始状态z1服从的高斯分布的均值和方差。
### 图模型
跟HMM一样

### 怎么求边缘后验概率
